{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82f730a8",
   "metadata": {},
   "source": [
    "# Einführung in Python für die Computational Social Science (CSS)\n",
    "\n",
    "## Jonas Volle\n",
    "Wissenschaftlicher Mitarbeiter  \n",
    "Chair of Methodology and Empirical Social Research  \n",
    "Otto-von-Guericke-Universität\n",
    "\n",
    "[jonas.volle@ovgu.de](mailto:jonas.volle@ovgu.de)\n",
    "\n",
    "**Sprechstunde**: individuell nach vorheriger Anmeldung per [Mail](mailto:jonas.volle@ovgu.de)\n",
    "\n",
    "Donnerstag, 04.07.2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e01ee4",
   "metadata": {},
   "source": [
    "**Quelle:** Ich orientiere mich für diese Sitzung in Teilen am Kapitel 7 aus dem Buch:  \n",
    "\n",
    "McLevey, John. 2021. Doing Computational Social Science: A Practical Introduction. 1st ed. Thousand Oaks: SAGE Publications.\n",
    "\n",
    "und der Introduction to Computational Social Science methods with Python von GESIS unter: https://github.com/gesiscss/css_methods_python "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f278555f",
   "metadata": {},
   "source": [
    "# Session 6: Textanalysen "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1d850b-c236-4185-be53-4a35fcf28a2b",
   "metadata": {},
   "source": [
    "## Import der Textdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4a671c-f1cc-403e-9ca8-2851f2430685",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cred\n",
    "import requests\n",
    "import pprint as pp\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "GUARDIAN_KEY = cred.GUARDIAN_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0c002a-be0e-4ee2-b837-20eac8a81de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Endpoint\n",
    "API_ENDPOINT = 'http://content.guardianapis.com/search'\n",
    "\n",
    "# API Parameter\n",
    "PARAMS = {\n",
    "    'api-key': GUARDIAN_KEY,\n",
    "    'from-date': '2024-01-01',\n",
    "    'to-date': '2024-06-30',\n",
    "    'lang': 'en',\n",
    "    'production-office': 'uk',\n",
    "    'q': '\"european union\" OR EU OR eurozone OR brussels OR \"european parliament\"',\n",
    "    'show-fields': 'wordcount,body,byline',\n",
    "    'page-size': 50\n",
    "} \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b1adc6-8dac-4041-b1e8-98b60bea67c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET request\n",
    "\n",
    "response = requests.get(API_ENDPOINT, params=PARAMS) \n",
    "response_dict = response.json()['response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85309904-654b-436d-8fd4-0f160c356270",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_dict['total']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6fa1d9-94a1-4926-aeae-b9fad2113305",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_dict['pages']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5207e0-7f69-4135-b829-23b6c614b896",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_results = []\n",
    "cur_page = 1\n",
    "total_pages = 1\n",
    "\n",
    "while (cur_page <= total_pages) and (cur_page < 50):\n",
    "\n",
    "    # Make API request\n",
    "    PARAMS['page'] = cur_page\n",
    "    response = requests.get(API_ENDPOINT, params=PARAMS) \n",
    "    response_dict = response.json()['response']\n",
    "\n",
    "    # update total pages\n",
    "    total_pages = response_dict['pages']\n",
    "\n",
    "    print(f\"page: {cur_page} of {total_pages}\")\n",
    "\n",
    "    # update cur page\n",
    "    cur_page += 1\n",
    "\n",
    "    # append result\n",
    "    all_results += (response_dict['results'])\n",
    "\n",
    "    # sleep\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d1a657-b389-410f-bfa2-7b48532de1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results_df = pd.json_normalize(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153036a1-3fb6-479d-a264-5e8bacf2aa02",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results_df['text'] = [BeautifulSoup(i, \"html.parser\").text for i in all_results_df['fields.body']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b7a9ba-8e36-4343-a49e-99fe3bca30b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# date\n",
    "all_results_df['article_date'] = pd.to_datetime(all_results_df.webPublicationDate)\n",
    "\n",
    "# rename columns\n",
    "all_results_df = all_results_df.rename(columns={'webTitle':'article_title',\n",
    "                                               'webUrl':'article_url',\n",
    "                                               'fields.byline': 'article_author',\n",
    "                                               'sectionName': 'section_name',\n",
    "                                               'pillarName': 'pillar_name'})\n",
    "\n",
    "# filter columns\n",
    "all_results_df_f = all_results_df[['id', 'article_date', 'section_name', 'pillar_name',\n",
    "                                   'article_title', 'article_url', \n",
    "                                   'article_author', 'text']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467d107e-3995-4b1a-8951-a07a18a02b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results_df_f.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba3337a-aa7e-49a1-8f27-81deacc51c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export full data\n",
    "# all_results_df_f.to_csv('../data/guardian_eu_textdata.csv', index= False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd49df05-c2d9-41f6-aa75-9be40b0223ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export sampled data\n",
    "\n",
    "all_results_df_sample_100 = all_results_df_f.sample(100, random_state=1234)\n",
    "# all_results_df_sample_100.to_csv('../data/guardian_eu_textdata_sample_100.csv',\n",
    "#                                  index= False)\n",
    "\n",
    "all_results_df_sample_200 = all_results_df_f.sample(200, random_state=1234)\n",
    "# all_results_df_sample_200.to_csv('../data/guardian_eu_textdata_sample_200.csv',\n",
    "#                                  index= False)\n",
    "\n",
    "all_results_df_sample_500 = all_results_df_f.sample(500, random_state=1234)\n",
    "# all_results_df_sample_500.to_csv('../data/guardian_eu_textdata_sample_500.csv',\n",
    "#                                  index= False)\n",
    "\n",
    "all_results_df_sample_1000 = all_results_df_f.sample(1000, random_state=1234)\n",
    "# all_results_df_sample_1000.to_csv('../data/guardian_eu_textdata_sample_1000.csv',\n",
    "#                                  index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd21164a",
   "metadata": {},
   "source": [
    "## Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53807a2e-da62-4a64-b284-f73d0e85bfa3",
   "metadata": {},
   "source": [
    "Zunächst werden die Textdaten importiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b462b7dc-c5bb-4195-95c7-b6ec5377bc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fe35a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/guardian_eu_textdata_sample_100.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd67437",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9bb6d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a0fc80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d4f0f2ef",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "Für die Analyse werden die Texte in Analyseeinheiten zerteilt.\n",
    "\n",
    "Die durch Leerzeichen und Interpunktion getrennten Wörter eines Textdokuments werden als Token bezeichnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c16f2ce-45f4-4663-b983-95d96d99c492",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710a55c7-b991-4fe2-97da-bb1b65681591",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee74f1bb-cff8-4e0d-9b3f-2ce52af0b373",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "233e7f3c-a0ee-483f-91cb-c19969ff8086",
   "metadata": {},
   "source": [
    "Wir können auch alle tokens in eine List packen, um die häufigsten token zu zählen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bd04ca-0b6e-48e7-af34-8a58b2d637bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Alle tokens in einer Liste\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41ff4ae-ed1e-4d18-8d69-7bd9f6567a56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02628ab1-58ba-48fd-912f-68aa377293e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(vocabulary.values(), bins=1000, color='blue', edgecolor='black')\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513c7896-9ad1-419e-8fd4-01f5fc75978e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1fbc8539-7a59-4a2d-803e-5928dd205062",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "Beim Stemming werden die Suffixe von Wörtern entfernt, um eine vereinfachte Form des Wortes zu erhalten.\n",
    "\n",
    "running, runner, run -> run\n",
    "\n",
    "Ein weit verbreiteter Stemming Algorithmus ist der von Porter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12585d85-01d6-4968-bc09-fdae43338bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5f505d-6084-4efd-a39b-ce31028987b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m nltk.downloader popular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a85f4a-4425-4cfb-96bc-fba155127a6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61068dbd-5a3a-4bca-831f-56fc8b7c304f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ed19f1-b232-48f9-82c1-5992e1da70f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_df = pd.DataFrame({'token': word_tokenize(example_text)})\n",
    "example_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3a54ac-a164-4518-8715-5a96bf46577c",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_df['stem'] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c7ab48-bf88-44b7-a96a-ab48b2b4e7ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b50dae8",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "\n",
    "Ein Lemma ist die Grundform eines Wortes.  \n",
    "\n",
    "go, goes, went, gone oder going --> go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c965a64-e8ee-457c-93af-7c617c56e42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import von spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e94c05-d98e-4a8b-8fe5-b08b6d950007",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bb36dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the text with spaCy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e300c190-876b-4fbe-8a06-d45bf096f561",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca25b6f4-9661-469c-b062-e750eb4f0599",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# https://spacy.io/api/token#attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4def5351",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bab2c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lemma_df = pd.DataFrame({'token': ,\n",
    "                         'lemma': })\n",
    "\n",
    "lemma_df['stem'] = [stemmer.stem(i) for i in lemma_df.token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb7339b-0828-4af4-8dbc-9f97c1845950",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_df.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0489a04c-2157-4f67-a142-5ed105f94cf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e1bf414c",
   "metadata": {},
   "source": [
    "### N-grams\n",
    "\n",
    "N-grams sind Kombinationen von n Wörtern. gensim kann Worte erkennen, die oft zusammen auftauchen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d644bd-9bd2-4e19-8b2d-86f784a78f68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4235b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gensim expect as input tokenized texts\n",
    "texts = [word_tokenize(text) for text in df.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6983abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract bigrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c5cd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the extracted bigrams\n",
    "extracted_bigrams = []\n",
    "for text in texts_bigrams:\n",
    "    for el in text:\n",
    "        if \"_\" in el:\n",
    "            extracted_bigrams.append(el)\n",
    "\n",
    "extracted_bigrams = set(extracted_bigrams)\n",
    "print(extracted_bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a4fd7b",
   "metadata": {},
   "source": [
    "### Stopwords\n",
    "\n",
    "Stoppwörter sind Wörter, die häufig in einer Sprache verwendet werden, aber normalerweise keine große Bedeutung oder keinen semantischen Wert haben, wenn sie im Kontext verwendet werden. Beispiele für Stoppwörter im Englischen sind \"the\", \"a\", \"an\", \"and\", \"in\", \"on\", \"is\", \"are\", \"for\", \"with\" und so weiter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36633231-9fa5-440d-9fbe-c79e7feac0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(vocabulary.values(), bins=1000, color='blue', edgecolor='black')\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e26526",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26aaaa77",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df.text[40]\n",
    "\n",
    "# Process the text with spaCy\n",
    "\n",
    "\n",
    "# Define the list of stop words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55964e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words from the text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006a5caa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print the original and filtered text, and the stop words removed\n",
    "print(\"Original tokens: \", [token.text for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf481433",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Filtered tokens:\", filtered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92becfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Stop words removed: \", stop_words_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87cde5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(stop_words))\n",
    "stop_words.extend([\"bst\"])\n",
    "print(len(stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92718018",
   "metadata": {},
   "outputs": [],
   "source": [
    "'bst' in stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb85003e",
   "metadata": {},
   "source": [
    "### Parts of Speech"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c687558a",
   "metadata": {},
   "source": [
    "English has 9 main categories:\n",
    "\n",
    "verb — Expresses an action or a state of being. E.g. jump, is, write, become  \n",
    "noun — identifies a person, a place or a thing or names of particular of one of these (pronoun). E.g. man, house, happiness  \n",
    "pronoun — can replace a noun or noun phrase. E.g. she, we, they, it  \n",
    "determiner — Is placed in front of a noun to express a quantity or clarify  what the noun refers to \n",
    "adjective — modifies a noun or a pronoun. E.g. pretty, old, blue, smart  \n",
    "adverb — modifies a verb, an adjective, or another adverb. E.g. gently, extremely, carefully, well  \n",
    "preposition — Connect a noun/pronoun to other parts of the sentence. E.g. by, with, about, until  \n",
    "conjunction — glue words, clauses, and sentences together. E.g. and, but, or, while, because  \n",
    "interjection — Expresses emotion in a sudden or exclamatory way. E.g. oh!, wow!, oops!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf587e38-6e94-4eff-b456-a39c1e455953",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'token': [token.text for token in doc],\n",
    "             'pos': [token.pos_ for token in doc]}).sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7adf18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.explain(\"PROPN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb4865d-bcc2-4b7b-9b58-39b50818db54",
   "metadata": {},
   "source": [
    "### Named enitity recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3841e939-3894-49e9-9d1c-6c44061d5fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text_entities = pd.DataFrame({'entity': ,\n",
    "                                      'entity_label': })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83548ad2-acc1-4bf3-8d26-b56f6f55935f",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text_entities[example_text_entities.entity_label == ''].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3226a90-e2df-4cf1-a293-66d4e466461e",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text_entities[example_text_entities.entity_label == ''].value_counts('entity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94b5642",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "05194ae4",
   "metadata": {},
   "source": [
    "### Preprocessing Pipeline\n",
    "\n",
    "Verschiedene Vorverarbeitungsschritte können wir in einer Pipeline an Funktionen zusammenfassen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba2b64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re # regex\n",
    "from nltk.tokenize import word_tokenize\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d002345",
   "metadata": {},
   "source": [
    "Sonderzeichen, Zahlen, Zeilenumbrüch etc. entfernen. In dieser Funktion benutzen wir reguläre Ausdrücke `regex`. Eine Übersicht über diese Ausdrücke finden wir z.B. hier: https://images.datacamp.com/image/upload/v1665049611/Marketing/Blog/Regular_Expressions_Cheat_Sheet.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3709cff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "\n",
    "    # remove punctuation and special characters\n",
    "    pattern = r\"[^\\w\\s]\"\n",
    "    text_clean = re.sub(pattern, \"\", text)\n",
    "\n",
    "    # remove numbers\n",
    "    pattern = r\"\\d+\"\n",
    "    text_clean = re.sub(pattern, \"\", text_clean)\n",
    "\n",
    "    # remove all non-ASCII characters\n",
    "    pattern = r\"[^\\x00-\\x7F]+\"\n",
    "    text_clean = re.sub(pattern, \"\", text_clean)\n",
    "\n",
    "    # remove new line characters\n",
    "    text_clean.replace(\"\\n\", \"\")\n",
    "\n",
    "    # remove empty spaces left by regex\n",
    "    text_clean = ' '.join(text_clean.split())\n",
    "    \n",
    "    return text_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e3266c",
   "metadata": {},
   "source": [
    "... Tokenisierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0634c7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(texts):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e87934",
   "metadata": {},
   "source": [
    "... Stopwords entfernen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84ba8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(texts, stop_words=[]):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97c0122",
   "metadata": {},
   "source": [
    "... Bigrams hinzufügen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb9525f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_bigrams(texts):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727c4d2d",
   "metadata": {},
   "source": [
    "... stemmen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0774f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(texts):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8280a9fc",
   "metadata": {},
   "source": [
    "... lemmatisieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f8e9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(texts):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2863c6",
   "metadata": {},
   "source": [
    "Alle Funktionen können wir jetzt in eine Pipeline integrieren. Diese Funktion nimmt einen Textkorpus auf. Ein Textkorpus besteht aus einer Reihe an Dokumenten. Diese Dokumente können z.B. in einer Liste oder einem array gespeichert sein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414f3b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = list(STOP_WORDS)\n",
    "stop_words.extend(['bst', 'year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9320ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(corpus):\n",
    "    print(\"Cleaning text...\")\n",
    "    corpus = [clean_text(text) for text in corpus]\n",
    "\n",
    "    print(\"Tokenization...\")\n",
    "    corpus = tokenization(corpus)\n",
    "\n",
    "    print(\"Lowercasing...\")\n",
    "    corpus = [[el.lower() for el in text] for text in corpus]\n",
    "\n",
    "    print(\"Stop Words removal...\")\n",
    "    corpus = remove_stop_words(corpus, stop_words=stop_words)\n",
    "    \n",
    "    print(\"Extract bigrams...\")\n",
    "    corpus = add_bigrams(corpus)\n",
    "\n",
    "    print(\"Lemmatization...\")\n",
    "    corpus = lemmatization(corpus)\n",
    "\n",
    "    print(\"Stop Words removal after lemmatizing...\")\n",
    "    corpus = remove_stop_words(corpus, stop_words)\n",
    "\n",
    "    print(\"Removing tokens that are too short...\")\n",
    "    corpus = [[c for c in text if len(c) > 2] for text in corpus]\n",
    "\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38799e2",
   "metadata": {},
   "source": [
    "Nun erstellen wir unseren Corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80ebffa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22cde8f-52fb-4d90-9efe-f323ec3cbdd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d72efb-f842-4bf0-9fad-41177ce49cf6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93629a31-d10f-477e-8167-cd7a0a03bed6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3728f5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d471c501-3c8e-41d3-a5b1-ee0d8a5e380e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we create a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678362c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of words in the dictionary: {len(dictionary)}\")\n",
    "print(\"Dictionary first 5 elements (id, token):\", list(dictionary.items())[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4c33fb-205c-44c8-9861-40a1e3a410cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(dictionary.dfs.values())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479b3fd8-eadc-4a6f-9cbe-215884f111eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prune the dictionary:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464b08b6-2b03-48de-8e09-1913d0189018",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of words in the dictionary after pruning: {len(dictionary)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72132bbd-c584-4474-9a97-92f9e437b85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(dictionary.dfs.values())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2db5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# covert the corpus to bag of words format \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8caf1ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First document in bag-of-words format (raw):\", document_term_matrix[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad15439",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First document in bag-of-words format (word, frequency):\",\n",
    "      [[dictionary[id], freq] for id, freq in document_term_matrix[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f316e096",
   "metadata": {},
   "source": [
    "Top words im Corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbfccc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts_df = pd.DataFrame([[dictionary[id], freq] for id, freq in dictionary.cfs.items()],\n",
    "                            columns=['word', 'count']).sort_values('count', ascending=False)\n",
    "\n",
    "word_counts_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a205555b-45e8-4c89-af6f-777df416c9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3956c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.barplot(data=top_words, \n",
    "                 y='word',\n",
    "                 x='count',\n",
    "                 color='darkgray') \n",
    "sns.despine() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6e5941-86d6-4aaf-a4d8-ff79050a823a",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts_df.tail(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18faf49d",
   "metadata": {},
   "source": [
    "### export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6007ce2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "with open(\"../data/dict_gensim.pkl\", \"wb\") as file:\n",
    "    pkl.dump(dictionary, file)\n",
    "\n",
    "with open(\"../data/text_df.pkl\", \"wb\") as file:\n",
    "    pkl.dump(df, file)\n",
    "\n",
    "with open(\"../data/document_term_matrix.pkl\", \"wb\") as file:\n",
    "    pkl.dump(document_term_matrix, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d3071a-7db3-4921-90bf-a3f03839b208",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-success'>\n",
    "\n",
    "### Aufgabe 1\n",
    "\n",
    "--> session_06_excercise_01.ipynb\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214698dc",
   "metadata": {},
   "source": [
    "## Topic Model\n",
    "\n",
    "Topic Models sind probabilistische Modelle, die zur Bestimmung von semantischen Clustern in Dokumentensammlungen verwendet werden. Sie eignen sich für die Erforschung von Textdaten, da sie thematische Strukturen finden, die nicht im Voraus definiert sind. Die Berechnung zielt darauf ab, die proportionale Zusammensetzung einer festen Anzahl von Themen in den Dokumenten einer Sammlung zu bestimmen. Diese semantischen Cluster können wir als Themen interpretieren.\n",
    "\n",
    "Topic Modelle liefern Wahrscheinlichkeitsverteilungen über die Menge aller Wörter für jedes Thema und Wahrscheinlichkeitsverteilungen über die Menge der Themen für jedes Dokument. Jede kleinste Analyseeinheit (z. B. ein Wort oder ein n-Gramm) hat eine Wahrscheinlichkeit, zu jedem Thema zu gehören, und jedes Thema hat eine Wahrscheinlichkeit, in jedem Dokument aufzutreten. Ein Thema wird semantisch interpretierbar durch die n wahrscheinlichsten Wörter, die es enthält."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886abd3e-ffb7-4a08-a820-41129bcebf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "# Load the gensim dictionary\n",
    "with open(\"../data/dict_gensim.pkl\", \"rb\") as file:\n",
    "    dictionary = pkl.load(file)\n",
    "\n",
    "# Load the DataFrame\n",
    "with open(\"../data/text_df.pkl\", \"rb\") as file:\n",
    "    df = pkl.load(file)\n",
    "\n",
    "# Load the document term matrix\n",
    "with open(\"../data/document_term_matrix.pkl\", \"rb\") as file:\n",
    "    document_term_matrix = pkl.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f33eb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# train an LDA model on the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8eb591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the topics and associated keywords\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c0d68c",
   "metadata": {},
   "source": [
    "Auswahl des Modells anhand des Kohärenz Scores. \n",
    "\n",
    "\n",
    "Die Topic Kohärenz bewerten ein einzelnes Topic, indem sie den Grad der semantischen Ähnlichkeit zwischen hoch bewerteten Wörtern im Thema messen. Diese Messungen helfen bei der Unterscheidung zwischen Themen, die semantisch interpretierbar sind, und Themen, die Artefakte statistischer Inferenz sind.  Zusätzlich können wir verschiedene Modelle mit dem Wert der mittleren Kohärenz vergleichen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd30e408",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "import numpy as np\n",
    "\n",
    "scores = []\n",
    "models = []\n",
    "\n",
    "for num_topics in np.arange(5, 30):\n",
    "\n",
    "    # fit LDA model\n",
    "    lda_model = LdaModel(document_term_matrix,\n",
    "                         id2word=dictionary,\n",
    "                         num_topics=num_topics,\n",
    "                         random_state=12345\n",
    "                        )\n",
    "\n",
    "    # compute Coherence Score\n",
    "    coherence_model_lda = CoherenceModel(model=lda_model,\n",
    "                                         texts=df.text_preprocessed,\n",
    "                                         dictionary=dictionary)\n",
    "    \n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "    print(f'Coherence Score with {num_topics} topics: {coherence_lda}')\n",
    "\n",
    "    scores.append([num_topics, coherence_lda])\n",
    "    models.append(lda_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e73316",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df = pd.DataFrame(scores, columns=['num_topic', 'coherence_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9065228e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.lineplot(data=scores_df, x='num_topic', y='coherence_score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552d777d-6d2e-4ac5-a2c8-f91051b80cbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores_df.sort_values('coherence_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fa158d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8882e3db-0db2-4d0e-8efe-865f102e7978",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92623bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "\n",
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim_models.prepare(lda_model_best,\n",
    "                                     document_term_matrix,\n",
    "                                     dictionary)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361fa2d5-89cc-403b-823b-c5a2e433496c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "325fac00-5c98-48f3-b256-6e4b70a4c539",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-success'>\n",
    "\n",
    "### Aufgabe 2\n",
    "\n",
    "--> session_06_excercise_02.ipynb\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778b0030-3dde-4ccd-8254-f1e967196e81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:python_course]",
   "language": "python",
   "name": "conda-env-python_course-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
